{
    "activation_dropout": 0.0,
    "activation_function": "gelu",
    "architectures": [
        "eBart"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 16385,
    "d_model": 1024,
    "decoder_attention_heads": 16,
    "decoder_ffn_dim": 2730,
    "decoder_layers": 12,
    "decoder_start_token_id": 16384,
    "do_sample": true,
    "dropout": 0.0,
    "encoder_attention_heads": 16,
    "encoder_ffn_dim": 2730,
    "encoder_layers": 12,
    "encoder_vocab_size": 50264,
    "eos_token_id": 16385,
    "force_ln_scale": false,
    "gradient_checkpointing": true,
    "image_length": 256,
    "image_vocab_size": 16384,
    "init_std": 0.02,
    "is_encoder_decoder": true,
    "ln_positions": "normformer",
    "ln_type": "layernorm",
    "max_length": 257,
    "max_text_length": 64,
    "min_length": 257,
    "model_type": "dallebart",
    "normalize_text": false,
    "pad_token_id": 16385,
    "scale_embedding": false,
    "sinkhorn_iters": 1,
    "tau_init": 0.05,
    "tie_word_embeddings": false,
    "transformers_version": "4.19.0.dev0",
    "use_absolute_position_embeddings": true,
    "use_alibi": false,
    "use_bias": false,
    "use_cache": true,
    "use_cosine_attention": false,
    "use_deepnet_scaling": false,
    "use_final_ln_decoder": true,
    "use_final_ln_encoder": true,
    "use_glu": true,
    "use_head_scale": false,
    "use_scan": true,
    "use_swin_position_embeddings": false
}